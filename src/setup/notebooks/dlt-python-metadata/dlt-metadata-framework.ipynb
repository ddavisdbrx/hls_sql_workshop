{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ccb72020-abf9-425f-994a-c3cc09abe79d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime\n",
    "from  pyspark.sql.functions import input_file_name as N\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "from pyspark.sql import functions as F\n",
    "import json\n",
    "#from pyspark.sql.functions import md5\n",
    "\n",
    "def dlt_framework(source_table,file_location,file_type,schema,lookup_keys,source_keys,source_sequence,drop_expectations):\n",
    "\n",
    "  @dlt.table(name=f\"framework_bronze.{source_table}\")\n",
    "  def bronze():\n",
    "    bronze_df = (spark.readStream.format('cloudFiles') \n",
    "                .option('cloudFiles.format', file_type) \n",
    "                .load('/Volumes/hls_de_workshop_dev/bronze/raw_data'+file_location) #need to figure out how to reference variable\n",
    "                .select(\"*\",\"_metadata\")\n",
    "                )\n",
    "\n",
    "    return bronze_df\n",
    "  \n",
    "\n",
    "  @dlt.table(name=f\"framework_silver.{source_table}_insert\")\n",
    "  def silver_insert():\n",
    "    silver_insert_df = (dlt.readStream(f\"framework_bronze.{source_table}\")\n",
    "            .withColumn(\"insert_timestamp\", current_timestamp())\n",
    "      )\n",
    "    sm = json.loads(schema)\n",
    "    #set the new name and data types for fields\n",
    "    for mapping in sm:\n",
    "      silver_insert_df = silver_insert_df.withColumnRenamed(mapping[\"source_column\"], mapping[\"new_column\"])\n",
    "      silver_insert_df = silver_insert_df.withColumn(mapping[\"new_column\"], silver_insert_df[mapping[\"new_column\"]].cast(mapping[\"data_type\"]))\n",
    "    \n",
    "    #use the lookups table to tranform fields to the lookup value\n",
    "    if lookup_keys != \"\":\n",
    "      lookups_df = spark.read.table(\"hls_de_workshop_dev.bronze.lookups\").drop(\"insert_timestamp\")\n",
    "      lookup_keys_str = lookup_keys\n",
    "      lookups = json.loads(lookup_keys_str)\n",
    "      for lookup in lookups:\n",
    "        for key, value in lookup.items():\n",
    "            silver_insert_df = silver_insert_df.join(\n",
    "                lookups_df,\n",
    "                (silver_insert_df[key] == lookups_df.code) & (lookups_df.variable == f'{key}'),\n",
    "                \"left\"\n",
    "            ).withColumn(f\"{value}\", lookups_df.label).withColumn(f\"{source_table}_key\", md5(col(source_keys)))\\\n",
    "            .drop(lookups_df.code, lookups_df.variable, lookups_df.label,lookups_df._rescued_data, lookups_df._metadata, silver_insert_df[key])\n",
    "\n",
    "    return silver_insert_df\n",
    "  \n",
    "  dlt.create_streaming_live_table(f\"framework_silver.{source_table}\")\n",
    "      \n",
    "  dlt.apply_changes(\n",
    "    target = f\"framework_silver.{source_table}\",\n",
    "    source = f\"framework_silver.{source_table}_insert\",\n",
    "    keys = [source_keys],\n",
    "    sequence_by = F.col(source_sequence),\n",
    "    stored_as_scd_type = 1\n",
    "  )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bbec1369-4bc3-45d4-aed1-854a4fbe03c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.sql(\"select * from hls_de_workshop_dev.metadata_framework.metadata where is_active = 1\")\n",
    "#df= df.withColumn(\"keyList\",split(col(\"keys\"),\",\"))\n",
    "\n",
    "\n",
    "for row in df.collect():\n",
    "  dlt_framework(row['table'],row['file_location'],row['file_type'],row['schema_mapping'],row['lookups'],row['keys'],row['sequence'],row['drop_expectations'])"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "dlt-metadata-framework",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
